{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sktime.utils.plotting import plot_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1 - preparation of the data\n",
    "* https://www.sktime.org/en/stable/examples/01_forecasting.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/later/profile_growth.csv\")\n",
    "\n",
    "#df.columns\n",
    "\n",
    "followers = df[['Date', 'Followers']]\n",
    "\n",
    "followers['Date'] = pd.PeriodIndex(pd.DatetimeIndex(followers['Date']), freq='D') \n",
    "\n",
    "y = followers.set_index('Date').sort_index()\n",
    "\n",
    "plot_series(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example, we will us the same airline data as in Section 1.2. But, instead of predicting the next 3 years, we hold out the last 3 years of the airline data (below: y_test), and see how the forecaster would have performed three years ago, when asked to forecast the most recent 3 years (below: y_pred), from the years before (below: y_train). “how” is measured by a quantitative performance metric (below: mean_absolute_percentage_error). This is then considered as an indication of how well the forecaster would perform in the coming 3 years (what was done in Section 1.2). This may or may not be a stretch depending on statistical assumptions and data properties (caution: it often is a stretch - past performance is in general not indicative of future performance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1 - splitting a historical data set in to a temporal train and test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "y_train, y_test = temporal_train_test_split(y, test_size=36)\n",
    "# we will try to forecast y_test from y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting for illustration\n",
    "plot_series(y_train, y_test, labels=[\"y_train\", \"y_test\"])\n",
    "print(y_train.shape[0], y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 2 - making forecasts for y_test from y_train\n",
    "This is almost verbatim the workflow in Section 1.2, using y_train to predict the indices of y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "# we can simply take the indices from `y_test` where they already are stored\n",
    "fh = ForecastingHorizon(y_test.index, is_relative=False)\n",
    "\n",
    "forecaster = NaiveForecaster(strategy=\"last\", sp=12)\n",
    "\n",
    "forecaster.fit(y_train)\n",
    "\n",
    "# y_pred will contain the predictions\n",
    "y_pred = forecaster.predict(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting for illustration\n",
    "plot_series(y_train, y_test, y_pred, labels=[\"y_train\", \"y_test\", \"y_pred\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### steps 3 and 4 - specifying a forecasting metric, evaluating on the test set\n",
    "The next step is to specify a forecasting metric. These are functions that return a number when input with prediction and actual series. They are different from sklearn metrics in that they accept series with indices rather than np.arrays. Forecasting metrics can be invoked in two ways:\n",
    "\n",
    "using the lean function interface, e.g., mean_absolute_percentage_error which is a python function (y_true : pd.Series, y_pred : pd.Series) -> float\n",
    "\n",
    "using the composable class interface, e.g., MeanAbsolutePercentageError, which is a python class, callable with the same signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.performance_metrics.forecasting import mean_absolute_percentage_error\n",
    "# option 1: using the lean function interface\n",
    "mean_absolute_percentage_error(y_test, y_pred)\n",
    "# note: the FIRST argument is the ground truth, the SECOND argument are the forecasts\n",
    "#       the order matters for most metrics in general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To properly interpret numbers like this, it is useful to understand properties of the metric in question (e.g., lower is better), and to compare against suitable baselines and contender algorithms (see step 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.performance_metrics.forecasting import MeanAbsolutePercentageError\n",
    "# option 2: using the composable class interface\n",
    "mape = MeanAbsolutePercentageError(symmetric=False)\n",
    "# the class interface allows to easily construct variants of the MAPE\n",
    "#  e.g., the non-symmetric verion\n",
    "# it also allows for inspection of metric properties\n",
    "#  e.g., are higher values better (answer: no)?\n",
    "mape.greater_is_better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation works exactly like in option 2, but with the instantiated object\n",
    "mape(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 5 - testing performance against benchmarks\n",
    "In general, forecast performances should be quantitatively tested against benchmark performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "instagram-growth-strategy",
   "language": "python",
   "name": "instagram-growth-strategy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
