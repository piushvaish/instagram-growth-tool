{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import filenames\n",
    "import csv\n",
    "import json\n",
    "import pickle\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score, roc_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "pd.set_option('display.max_rows', 500, 'display.max_columns', 500,\n",
    "              'display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "use glob to get all the csv files in the raw data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_files = filenames.profile_folder_path.glob(os.path.join(\"*.csv\"))\n",
    "\n",
    "profile_appended_data = []\n",
    "# loop over the list of csv files\n",
    "for f in profile_files:\n",
    "    data = pd.read_csv(f)\n",
    "    profile_appended_data.append(data)\n",
    "#profile_appended_data\n",
    "\n",
    "df = pd.concat(profile_appended_data)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop duplicate userid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['userid'], keep='last').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Label for Followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = filenames.followers_path\n",
    "follower = []\n",
    "with open(fpath, newline='') as f:\n",
    "    for i in csv.reader(f):\n",
    "        follower.append(i[0])\n",
    "df['is_follower'] = df['username'].isin(follower).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\n",
    "    'userid','followed_by_viewer', 'igtvcount', 'blocked_by_viewer',\n",
    "    'follows_viewer', 'has_blocked_viewer', 'has_requested_viewer', 'external_url',\n",
    "    'is_verified', 'requested_by_viewer', 'profile_pic_url', 'similar_accounts', 'business_category_name', 'biography', 'full_name'\n",
    "],\n",
    "        axis=1,\n",
    "        inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To share with others to try the code in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"../../data/deidentified_profile_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boxplot using plotly\n",
    "* To do - add to the dash application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Box(\n",
    "    y=df1['mediacount'],\n",
    "    name=\"Media Count\",\n",
    "    jitter=0.3,\n",
    "    pointpos=-1.8,\n",
    "    boxpoints='all', # represent all points\n",
    "    marker_color='rgb(7,40,89)',\n",
    "    line_color='rgb(7,40,89)'\n",
    "))\n",
    "\n",
    "\n",
    "fig.add_trace(go.Box(\n",
    "    y=df1['followees'],\n",
    "    name=\"Followees\",\n",
    "    boxpoints='suspectedoutliers', # only suspected outliers\n",
    "    marker=dict(\n",
    "        color='rgb(8,81,156)',\n",
    "        outliercolor='rgba(219, 64, 82, 0.6)',\n",
    "        line=dict(\n",
    "            outliercolor='rgba(219, 64, 82, 0.6)',\n",
    "            outlierwidth=2)),\n",
    "    line_color='rgb(8,81,156)'\n",
    "))\n",
    "\n",
    "fig.update_layout(title_text=\"Box Plot Media Count & Followees\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Box(\n",
    "    y=df1['followers'],\n",
    "    name=\"Followers\",\n",
    "    boxpoints=False, # no data points\n",
    "    marker_color='rgb(9,56,125)',\n",
    "    line_color='rgb(9,56,125)'\n",
    "))\n",
    "\n",
    "\n",
    "fig.update_layout(title_text=\"Box Plot Followers\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Box(\n",
    "    y=df1['mediacount'],\n",
    "    x=df1['is_follower'],\n",
    "    name='Media Count',\n",
    "    marker_color='#3D9970'\n",
    "))\n",
    "fig.add_trace(go.Box(\n",
    "    y=df1['followees'],\n",
    "    x=df1['is_follower'],\n",
    "    name='Followees',\n",
    "    marker_color='#FF4136'\n",
    "))\n",
    "fig.add_trace(go.Box(\n",
    "    y=df1['followers'],\n",
    "    x=df1['is_follower'],\n",
    "    name='Followers',\n",
    "    marker_color='#FF851B'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    yaxis_title='Count',\n",
    "    boxmode='group' # group together boxes of the different traces for each value of x\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['is_follower'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skewed and Imbalanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = shuffle(df1)\n",
    "df1.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.replace({False: 0, True: 1}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Split Params\n",
    "test_size = 0.33\n",
    "random_state = 42\n",
    "\n",
    "X = df1.drop(['is_follower'], axis = 1)\n",
    "y = df1['is_follower'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "\n",
    "print(f'Train dataset size: {X_train.shape[0]} \\n')\n",
    "print(f'Test dataset size: {X_test.shape[0]}')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop username\n",
    "Username is joined to predictions for batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['username'], axis = 1)\n",
    "X_test_new = X_test.drop(['username'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "The best practice is to imagine you have deployed your model, and it is being used to predict things. Imagine a single test case is provided to your model for testing, or your model tends to predict an input after deployment. \n",
    "\n",
    "In this scenario, you only have a single input, and therefore it doesn't make sense to use it as fitting data for a Standard Scaler which gets this single instance as its training (fitting) data, because in this case, the output of the scaler would be different in terms of scale with every other input. \n",
    "\n",
    "Thus, you'd better train (fit) the Standard Scaler using the training set (i.e. after splitting) and transform the rest of data (including the validation set, your test set, and whatever data comes into your model after deployment) using the fitted Standard Scaler.\n",
    "\n",
    "Moreover, in every stage of a machine learning project, you'd better use only the training data for fitting and training whatever you need (e.g. scalers, predictors, regressors, etc.) and leave the validation and test data only for validation and testing.\n",
    "\n",
    "For cross-validation case, you'd better fit a scaler and transform your data within cross-validation, but it generally doesn't make much difference. You can test it though.\n",
    "\n",
    "#### Why Pipelines?\n",
    "The machine learning workflow consists of many steps from data preparation (e.g., dealing with missing values, scaling/encoding, feature extraction). \n",
    "\n",
    "When first learning this workflow, we perform the data preparation one step at a time. This can become time consuming since we need to perform the preparation steps to both the training and testing data. \n",
    "\n",
    "Pipelines allow us to streamline this process by compiling the preparation steps while easing the task of model tuning and monitoring. Scikit-Learn’s Pipeline class provides a structure for applying a series of data transformations followed by an estimator \n",
    "\n",
    "[link](https://scikit-learn.org/stable/common_pitfalls.html)\n",
    "[link](https://towardsdatascience.com/machine-learning-pipelines-with-scikit-learn-d43c32a6aa52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_selector = ['mediacount','followers','followees']\n",
    "ordinal_selector = ['is_private', 'is_business_account', 'has_public_story']\n",
    "\n",
    "num_processor = StandardScaler()\n",
    "ordinal_processor = OrdinalEncoder()\n",
    "\n",
    "preprocess = make_column_transformer(\n",
    "    (ordinal_processor, ordinal_selector),\n",
    "    (num_processor, num_selector) \n",
    "            \n",
    ")\n",
    "\n",
    "X_train = preprocess.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform test set\n",
    "X_test_new = preprocess.transform(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers \n",
    "\n",
    "Not all data is normal or normal enough to treat it as being drawn from a Gaussian distribution. A good statistic for summarizing a non-Gaussian distribution sample of data is the Interquartile Range, or IQR for short. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsupervised Outlier Detection using the Local Outlier Factor (LOF).\n",
    "\n",
    "The anomaly score of each sample is called the Local Outlier Factor. It measures the local deviation of the density of a given sample with respect to its neighbors. \n",
    "\n",
    "It is local in that the anomaly score depends on how isolated the object is with respect to the surrounding neighborhood. \n",
    "\n",
    "More precisely, locality is given by k-nearest neighbors, whose distance is used to estimate the local density. \n",
    "\n",
    "By comparing the local density of a sample to the local densities of its neighbors, one can identify samples that have a substantially lower density than their neighbors. \n",
    "\n",
    "These are considered outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify outliers in the training dataset\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "# summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced Dataset\n",
    "\n",
    "Some common over-sampling and under-sampling techniques in imbalanced-learn are imblearn.over_sampling.RandomOverSampler, imblearn.under_sampling.RandomUnderSampler, and imblearn.SMOTE. \n",
    "\n",
    "For these libraries there is a nice parameter that allows the user to change the sampling ratio.\n",
    "\n",
    "For example, in SMOTE, to change the ratio you would input a dictionary, and all values must be greater than or equal to the largest class (since SMOTE is an over-sampling technique). \n",
    "\n",
    "The reason I have found SMOTE to be a better fit for model performance is probably because with RandomOverSampler you are duplicating rows, which means the model can start to memorize the data rather than generalize to new data. \n",
    "\n",
    "SMOTE uses the K-Nearest-Neighbors algorithm to make \"similar\" data points to those under sampled ones.\n",
    "\n",
    "It is not good practice to blindly use SMOTE, setting the ratio to it's default (even class balance) because the model may overfit one or more of the minority classes (even though SMOTE is using nearest neighbors to make \"similar\" observations). \n",
    "\n",
    "In a similar way that you tune hyperparameters of a ML model you will tune the hyperparameters of the SMOTE algorithm, such as the ratio and/or knn. \n",
    "\n",
    "NOTE: It is vital that you do not use SMOTE on the full data set. You MUST use SMOTE on the training set only (after you split). Then validate on your val/test sets and see if your SMOTE model out performed your other model(s). \n",
    "\n",
    "If you do not do this there will be data leakage and your model is essentially cheating.\n",
    "\n",
    "[link](https://www.analyticsvidhya.com/blog/2020/10/overcoming-class-imbalance-using-smote-techniques/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total of 1 before SMOTE\n",
    "y_train.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=0, n_jobs=8 , sampling_strategy='minority', k_neighbors=7)\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total of 1 after SMOTE\n",
    "y_train.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling\n",
    "\n",
    "Training a model simply means learning (determining) good values for all the weights and the bias from labeled examples. \n",
    "\n",
    "In supervised learning, a machine learning algorithm builds a model by examining many examples and attempting to find a model that minimizes loss; this process is called empirical risk minimization.\n",
    "\n",
    "Loss is the penalty for a bad prediction. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater. \n",
    "\n",
    "The goal of training a model is to find a set of weights and biases that have low loss, on average, across all examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "# Fit on the training data\n",
    "gnb_model = gnb.fit(X_train, y_train)\n",
    "# Predict on the testing data\n",
    "predictions=gnb_model.predict(X_test_new)\n",
    "probabilities = gnb_model.predict_proba(X_test_new)[:,1]\n",
    "# Calculate the roc-auc score\n",
    "auc_nb=metrics.roc_auc_score(y_test, predictions)\n",
    "acc_nb = metrics.accuracy_score(y_test, predictions)\n",
    "f1_nb = metrics.f1_score(y_test, predictions)\n",
    "# Display\n",
    "print('F1 Score', \"%.4f\" % round(f1_nb,4))\n",
    "print('Accuracy', \"%.4f\" % round(acc_nb,4))\n",
    "print('AUC Score', \"%.4f\" % round(auc_nb,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "# Fit on the training data\n",
    "log_model=logreg.fit(X_train, y_train)\n",
    "# Predict on the testing data\n",
    "predictions=log_model.predict(X_test_new)\n",
    "probabilities = log_model.predict_proba(X_test_new)[:,1]\n",
    "# Calculate the roc-auc score\n",
    "auc_log=metrics.roc_auc_score(y_test, predictions)\n",
    "acc_log = metrics.accuracy_score(y_test, predictions)\n",
    "f1_log = metrics.f1_score(y_test, predictions)\n",
    "# Display\n",
    "print('F1 Score', \"%.4f\" % round(f1_log,4))\n",
    "print('Accuracy', \"%.4f\" % round(acc_log,4))\n",
    "print('AUC Score', \"%.4f\" % round(auc_log,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "# Fit on the training data\n",
    "knn_model=knn.fit(X_train, y_train)\n",
    "# Predict on the testing data\n",
    "predictions=knn_model.predict(X_test_new)\n",
    "probabilities = knn_model.predict_proba(X_test_new)[:,1]\n",
    "# Calculate the roc-auc score\n",
    "auc_knn=metrics.roc_auc_score(y_test, predictions)\n",
    "acc_knn = metrics.accuracy_score(y_test, predictions)\n",
    "f1_knn = metrics.f1_score(y_test, predictions)\n",
    "# Display\n",
    "print('F1 Score', \"%.4f\" % round(f1_knn,4))\n",
    "print('Accuracy', \"%.4f\" % round(acc_knn,4))\n",
    "print('AUC Score', \"%.4f\" % round(auc_knn,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "# Fit on the training data\n",
    "rf_model=rf.fit(X_train, y_train)\n",
    "# Predict on the testing data\n",
    "predictions=rf_model.predict(X_test_new)\n",
    "probabilities = rf_model.predict_proba(X_test_new)[:,1]\n",
    "# Calculate the roc-auc score\n",
    "auc_rf=metrics.roc_auc_score(y_test, predictions)\n",
    "acc_rf = metrics.accuracy_score(y_test, predictions)\n",
    "f1_rf = metrics.f1_score(y_test, predictions)\n",
    "# Display\n",
    "print('F1 Score', \"%.4f\" % round(f1_rf,4))\n",
    "print('Accuracy', \"%.4f\" % round(acc_rf,4))\n",
    "print('AUC Score', \"%.4f\" % round(auc_rf,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HistGradientBoostingClassifier\n",
    "\n",
    "[link](https://machinelearningmastery.com/histogram-based-gradient-boosting-ensembles/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = HistGradientBoostingClassifier()\n",
    "# Fit on the training data\n",
    "hist_model=hist.fit(X_train, y_train)\n",
    "# Predict on the testing data\n",
    "predictions=hist_model.predict(X_test_new)\n",
    "probabilities = hist_model.predict_proba(X_test_new)[:,1]\n",
    "# Calculate the roc-auc score\n",
    "auc_hist=metrics.roc_auc_score(y_test, predictions)\n",
    "acc_hist = metrics.accuracy_score(y_test, predictions)\n",
    "f1_hist = metrics.f1_score(y_test, predictions)\n",
    "# Display\n",
    "print('F1 Score', \"%.4f\" % round(f1_hist,4))\n",
    "print('Accuracy', \"%.4f\" % round(acc_hist,4))\n",
    "print('AUC Score', \"%.4f\" % round(auc_hist,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of 5 different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists from the metrics we produced.\n",
    "f1=[f1_nb, f1_log, f1_knn, f1_rf, f1_hist]\n",
    "acc=[acc_nb, acc_log, acc_knn, acc_rf, acc_hist]\n",
    "auc=[auc_nb, auc_log, auc_knn, auc_rf, auc_hist]\n",
    "\n",
    "# Define a function that will round our metrics.\n",
    "def rounder(metric):\n",
    "    scores_list=[]\n",
    "    for score in metric:\n",
    "        scores_list.append(round(float(score*100),1))\n",
    "    return scores_list\n",
    "\n",
    "# Apply it to each of the three lists.\n",
    "f1_scores=rounder(f1)\n",
    "acc_scores=rounder(acc)\n",
    "auc_scores=rounder(auc)\n",
    "score_types=['F1 score', 'Accuracy', 'AUC score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of model metrics\n",
    "models=['naive bayes', 'logistic regression', 'k-nearest neighbors', 'random forest', 'hist gradient boosting']\n",
    "index=['F1 score', 'Accuracy', 'AUC score']\n",
    "compare_models=pd.DataFrame([f1_scores, acc_scores, auc_scores], index=index, columns=models)\n",
    "compare_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv, for later use by plotly dash app.\n",
    "compare_models.to_csv('../../resources/compare_models.csv', index=True)\n",
    "pd.read_csv('../../resources/compare_models.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's display that with plotly.\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=compare_models.loc['F1 score'].index,\n",
    "    y=compare_models.loc['F1 score'],\n",
    "    name=compare_models.index[0],\n",
    "    marker_color='rgb(107,174,214)'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=compare_models.loc['Accuracy'].index,\n",
    "    y=compare_models.loc['Accuracy'],\n",
    "    name=compare_models.index[1],\n",
    "    marker_color='rgba(219, 64, 82, 0.6)'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=compare_models.loc['AUC score'].index,\n",
    "    y=compare_models.loc['AUC score'],\n",
    "    name=compare_models.index[2],\n",
    "    marker_color='rgb(7,40,89)'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Comparison of Possible Models',\n",
    "    xaxis = dict(title = 'Predictive models'), # x-axis label\n",
    "    yaxis = dict(title = 'Score'), # y-axis label\n",
    "    \n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest has the best performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Random Forest Using Grid Search\n",
    "\n",
    "When creating a machine learning model, you'll be presented with design choices as to how to define your model architecture. Often times, we don't immediately know what the optimal model architecture should be for a given model, and thus we'd like to be able to explore a range of possibilities. \n",
    "\n",
    "In true machine learning fashion, we'll ideally ask the machine to perform this exploration and select the optimal model architecture automatically. \n",
    "\n",
    "Parameters which define the model architecture are referred to as hyperparameters and thus this process of searching for the ideal model architecture is referred to as hyperparameter tuning.\n",
    "\n",
    "These hyperparameters might address model design questions such as:\n",
    "\n",
    "* What degree of polynomial features should I use for my linear model?\n",
    "* What should be the maximum depth allowed for my decision tree?\n",
    "* What should be the minimum number of samples required at a leaf node in my decision tree?\n",
    "* How many trees should I include in my random forest?\n",
    "* How many neurons should I have in my neural network layer?\n",
    "* How many layers should I have in my neural network?\n",
    "* What should I set my learning rate to for gradient descent?\n",
    "\n",
    "Hyperparameters are not model parameters and they cannot be directly trained from the data. Model parameters are learned during training when we optimize a loss function using something like gradient descent.\n",
    "\n",
    "Whereas the model parameters specify how to transform the input data into the desired output, the hyperparameters define how our model is actually structured.\n",
    "\n",
    "In general, this process includes:\n",
    "1. Define a model\n",
    "2. Define the range of possible values for all hyperparameters\n",
    "3. Define a method for sampling hyperparameter values\n",
    "4. Define an evaluative criteria to judge the model\n",
    "5. Define a cross-validation method\n",
    "\n",
    "#### Grid search\n",
    "Grid search is arguably the most basic hyperparameter tuning method. With this technique, we simply build a model for each possible combination of all of the hyperparameter values provided, evaluating each model, and selecting the architecture which produces the best results.\n",
    "\n",
    "* https://www.jeremyjordan.me/hyperparameter-tuning/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc=RandomForestClassifier(random_state=42)\n",
    "param_grid = { \n",
    "    'n_estimators': [10, 100],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Create grid search using 5-fold cross validation\n",
    "grid_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5,  n_jobs = 1, verbose=0)\n",
    "grid_rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_rfc.best_params_)\n",
    "model = grid_rfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on the testing data\n",
    "\n",
    "After the model is built, testing data once again validates that it can make accurate predictions. If training and validation data include labels to monitor performance metrics of the model, the testing data should be unlabeled. Test data provides a final, real-world check of an unseen dataset to confirm that the ML algorithm was trained effectively.\n",
    "\n",
    "* https://www.applause.com/blog/training-data-validation-data-vs-test-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.predict(X_test_new)\n",
    "probabilities = model.predict_proba(X_test_new)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the final model for use in the plotly dash app.\n",
    "file = open('../../resources/final_model.pkl', 'wb')\n",
    "pickle.dump(model, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full list of metrics\n",
    "def model_metrics(y_test, predictions):\n",
    "    '''\n",
    "    Calculate 5 standard model metrics\n",
    "    Return a dictionary with the metrics\n",
    "    '''\n",
    "    f1 = metrics.f1_score(y_test, predictions)\n",
    "    accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "    error = 1 - accuracy\n",
    "    precision = metrics.precision_score(y_test, predictions)\n",
    "    recall = metrics.recall_score(y_test, predictions)\n",
    "    rocauc =  metrics.roc_auc_score(y_test, predictions)\n",
    "    return {'precision': precision, 'recall': recall,'f1 score':f1, 'accuracy': accuracy, 'error rate': error,  'ROC-AUC': rocauc}\n",
    "\n",
    "eval_scores=model_metrics(y_test, predictions)\n",
    "eval_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round the y values.\n",
    "y_vals=[]\n",
    "for val in list(eval_scores.values()):\n",
    "    y_vals.append(round(float(val*100),1))\n",
    "y_vals    \n",
    "# Write over the previous dictionary with the rounded values.\n",
    "eval_scores=dict(zip(eval_scores.keys(), y_vals))\n",
    "print(eval_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now save that dictionary to a pickle file, for later use in plotly dash app\n",
    "file = open('../../resources/eval_scores.pkl', 'wb')\n",
    "pickle.dump(eval_scores, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And here's a reminder of how to read that back in again, just in case this is unfamiliar:\n",
    "file = open('../../resources/eval_scores.pkl', 'rb')\n",
    "evals=pickle.load(file)\n",
    "file.close()\n",
    "evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert that into a visualization.\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=list(evals.keys()),\n",
    "    y=list(evals.values())\n",
    "))\n",
    "\n",
    "fig.update_traces(marker_color='rgb(107,174,214)', marker_line_color='rgb(8,48,107)',\n",
    "                  marker_line_width=1.5, opacity=0.6)\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Evaluation Metrics for Random Forest Model (Testing Dataset = 578 profiles)',\n",
    "    xaxis = {'title': 'Metrics'},\n",
    "    yaxis = {'title': 'Percent'}, \n",
    "\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precison and Recall\n",
    "\n",
    "Precision is how many of the returned hits were true positive i.e. how many of the found were correct hits.\n",
    "Recall literally is how many of the true positives were recalled (found), i.e. how many of the correct hits were also found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPR, TPR, _ = roc_curve(y_test, probabilities)\n",
    "#FPR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC curve\n",
    "An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:\n",
    "* True Positive Rate\n",
    "* False Positive Rate\n",
    "\n",
    "True Positive Rate (TPR) is a synonym for recall.\n",
    "\n",
    "An ROC curve plots TPR vs. FPR at different classification thresholds. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives. \n",
    "\n",
    "[link](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc#:~:text=An%20ROC%20curve%20(receiver%20operating,False%20Positive%20Rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_score=round(100*roc_auc_score(y_test, predictions),1)\n",
    "roc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_dict={'FPR':list(FPR), \n",
    "          'TPR':list(TPR),\n",
    "          'y_test':[int(i) for i in y_test], \n",
    "          'predictions':[int(i) for i in predictions]\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save everything we need to reproduce the ROC-AUC figure in plotly dash.\n",
    "with open('../../resources/roc_dict.json', 'w') as f:\n",
    "    json.dump(roc_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../resources/roc_dict.json') as json_file:\n",
    "    roc_dict = json.load(json_file)\n",
    "FPR=roc_dict['FPR']\n",
    "TPR=roc_dict['TPR']\n",
    "y_test=pd.Series(roc_dict['y_test'])\n",
    "predictions=roc_dict['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC-AUC figure\n",
    "roc_score=round(100*roc_auc_score(y_test, predictions),1)\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "        x=FPR, \n",
    "        y=TPR,\n",
    "        mode='lines',\n",
    "        name=f'AUC: {roc_score}',\n",
    "        marker_color='rgb(150,150,150)'\n",
    "        ))\n",
    "fig.add_trace(go.Scatter(\n",
    "        x=[0,1], \n",
    "        y=[0,1],\n",
    "        mode='lines',\n",
    "        name='Baseline Area: 50.0',\n",
    "        marker_color='rgb(37,37,37)'\n",
    "        ))\n",
    "fig.update_layout(\n",
    "    title='Receiver Operating Characteristic (ROC): Area Under Curve',\n",
    "    xaxis={'title': 'False Positive Rate (100-Specificity)','scaleratio': 1,'scaleanchor': 'y'},\n",
    "    yaxis={'title': 'True Positive Rate (Sensitivity)'}\n",
    "    )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "\n",
    "A confusion matrix is a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A confusion matrix tells us our false positives and false negatives:\n",
    "matrix=confusion_matrix(y_test, predictions)\n",
    "print(matrix)\n",
    "cm=pd.DataFrame(matrix, columns=['pred: follower', 'pred: non-follower'])\n",
    "cm[f'n={len(y_test)}']=['actual: follower', 'actual: non-follower']\n",
    "cm=cm[[f'n={len(y_test)}', 'pred: follower', 'pred: non-follower']]\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cm dataframe to a pickle file, for later use in plotly dash app\n",
    "cm.to_csv('../../resources/confusion_matrix.csv', index=False)\n",
    "cm=pd.read_csv('../../resources/confusion_matrix.csv')\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix as a formatted table with Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Table(\n",
    "    header=dict(values=cm.columns,\n",
    "                line = dict(color='rgb(150,150,150)'),\n",
    "                fill = dict(color='rgb(150,150,150)'),\n",
    "                align = ['left'] * 5),\n",
    "    cells=dict(values=[cm[f'n={len(y_test)}'], cm['pred: follower'], cm['pred: non-follower']],\n",
    "               line = dict(color='#7D7F80'),\n",
    "               fill = dict(color='white'),\n",
    "               align = ['left'] * 5)))\n",
    "\n",
    "fig.update_layout(\n",
    "    title = f'Confusion Matrix: Random Forest Model (Testing Dataset)'\n",
    ")\n",
    "\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Feature Importance\n",
    "\n",
    "The feature importance (variable importance) describes which features are relevant. It can help with better understanding of the solved problem and sometimes lead to model improvements by employing the feature selection.\n",
    "\n",
    "The Random Forest algorithm has built-in feature importance which can be computed in two ways:\n",
    "\n",
    "##### Gini importance (or mean decrease impurity), which is computed from the Random Forest structure. \n",
    "\n",
    "Let’s look how the Random Forest is constructed. It is a set of Decision Trees. \n",
    "\n",
    "Each Decision Tree is a set of internal nodes and leaves. In the internal node, the selected feature is used to make decision how to divide the data set into two separate sets with similars responses within. \n",
    "\n",
    "The features for internal nodes are selected with some criterion, which for classification tasks can be gini impurity or infomation gain, and for regression is variance reduction. We can measure how each feature decrease the impurity of the split (the feature with highest decrease is selected for internal node). \n",
    "\n",
    "For each feature we can collect how on average it decreases the impurity. The average over all trees in the forest is the measure of the feature importance.\n",
    "\n",
    "This biggest advantage of this method is a speed of computation - all needed values are computed during the Radom Forest training. The drawbacks of the method is to tendency to prefer (select as important) numerical features and categorical features with high cardinality. What is more, in the case of correlated features it can select one of the feature and neglect the importance of the second one. \n",
    "\n",
    "##### Mean Decrease Accuracy - is a method of computing the feature importance on permuted out-of-bag (OOB) samples based on mean decrease in the accuracy. \n",
    "\n",
    "This method is not implemented in the scikit-learn package. The very similar to this method is [permutation based importance](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance).\n",
    "\n",
    "The permutation based importance is computationally expensive. The permutation based method can have problem with highly-correlated features, it can report them as unimportant.\n",
    "\n",
    "Feature Importance Computed with SHAP Values\n",
    "model-agnostic and works well with algorithms not from scikit-learn: Xgboost, Neural Networks (keras+tensorflow), LigthGBM, CatBoost. It can provide more information like decision plots or dependence plots.\n",
    "\n",
    "[link](https://mljar.com/blog/feature-importance-in-random-forest/#:~:text=Random%20Forest%20Built%2Din%20Feature%20Importance&text=It%20is%20a%20set%20of,sets%20with%20similars%20responses%20within.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (Random Forest)\n",
    "coeffs1=pd.DataFrame(list(zip(list(X_test.columns), model.best_estimator_.feature_importances_)), columns=['feature', 'coefficient'])\n",
    "coeffs=coeffs1.sort_values(by='coefficient', ascending=False)\n",
    "\n",
    "# Format the coefficients.\n",
    "y_vals=[]\n",
    "for val in list(coeffs['coefficient']):\n",
    "    y_vals.append(round(float(val),2))\n",
    "y_vals\n",
    "\n",
    "coeffs['coefficient']=y_vals\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results to a csv file, for later use by plotly dash app.\n",
    "coeffs.to_csv('../../resources/coefficients.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's display that with Plotly.\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=coeffs['feature'],\n",
    "    y=coeffs['coefficient']\n",
    "))\n",
    "\n",
    "fig.update_traces(marker_color='rgb(158,202,225)', marker_line_color='rgb(8,48,107)',\n",
    "                  marker_line_width=1.5, opacity=0.6)\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Number of Followers is a good indication of becoming a follower than a business account ',\n",
    "    xaxis = {'title': 'Instagram Features'},\n",
    "    yaxis = {'title': 'Odds of Becoming a Follower'}, \n",
    "\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(probabilities))\n",
    "print(len(predictions))\n",
    "print(len(y_test))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge usernames to probabilities\n",
    "\n",
    "One of the most important things you can do before deploying a model is try to understand model drift in an offline environment. \n",
    "\n",
    "Data scientists should seek to answer the question \"If I train a model using this set of features on data from six months ago, and I apply it to data that I generated today, how much worse is the model than the one that I created untrained off of data from a month ago and applied to today?\". \n",
    "\n",
    "Performing this analysis offline allows you to estimate the rate at which a model’s performance falls off and how often you’ll need to retrain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs=pd.DataFrame(probabilities, columns=['follower_probability'])\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_df = pd.DataFrame(y_test, columns=['actual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reset_index(drop = True)\n",
    "X_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['is_private'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.concat([X_test, probs, actual_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to display in plotly dash app\n",
    "final.to_csv('../../resources/final_probs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata=final.drop(['follower_probability'], axis=1)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Table(\n",
    "        header=dict(values=list(mydata.columns)),\n",
    "        cells=dict(values=list(mydata.loc[5]))))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the selected model on whole dataset before putting into production\n",
    "\n",
    "Theoretically, the more data your deployed model has seen, the better is should generalise. So if you trained the model on the full set of data you have available, it should generalise better than a model which only saw for example train/val sets (e.g. ~ 90%) from the full data set.\n",
    "\n",
    "For a given model (a functional form), changing the sample size will only affect Var(f^(x0)); namely, increasing the sample will diminish it. Meanwhile, Bias2(f^(x0)) will stay the same as the functional form f^(⋅) is fixed. (Clearly, the irreducible error also stays the same.)\n",
    "\n",
    "You reduce the expected squared error by reestimating the chosen model on the full sample as compared to having estimated it on just the training sample.)\n",
    "\n",
    "Unless you're limiting yourself to a simple class of convex models/loss functions, you're considerably better off keeping a final test split.\n",
    "\n",
    "[link](https://stats.stackexchange.com/questions/225820/is-it-needed-to-train-the-selected-model-again-on-entire-data-before-putting-in)\n",
    "\n",
    "[link](https://datascience.stackexchange.com/questions/33008/is-it-always-better-to-use-the-whole-dataset-to-train-the-final-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = X.drop('username', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess on all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = preprocess.fit_transform(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to use later for individual datapoint or for batch\n",
    "joblib.dump(preprocess, \"../../resources/preprocess.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit model on all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_full, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "Machine learning prediction and inference are two different aspects of machine learning. \n",
    "\n",
    "Prediction is the ability to accurately predict a response variable while inference deals with understanding the relationship between predictor variables and response variables. The difference in prediction vs inference models can be seen in examples such as predicting marketing campaign success or understanding how media influences sales on promotions, etc. \n",
    "\n",
    "Inference: You want to find out what the effect of Age, Passenger Class and, Gender has on surviving the Titanic Disaster. You can put up a logistic regression and infer the effect each passenger characteristic has on survival rates.\n",
    "\n",
    "Prediction: Given some information on a Titanic passenger, you want to choose from the set {lives,dies} and be correct as often as possible. \n",
    "\n",
    "You may use linear regression for an inference model while non-linear methods work best when prediction is your objective.\n",
    "\n",
    "[link](https://stats.stackexchange.com/questions/244017/what-is-the-difference-between-prediction-and-inference)\n",
    "[link](https://cloud.google.com/ai-platform/prediction/docs/overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use glob to get all the csv files in the raw data folder.\n",
    "prediction_files = filenames.others_profile_folder_path.glob(os.path.join(\"*.csv\"))\n",
    "\n",
    "prediction_appended_data = []\n",
    "# loop over the list of csv files\n",
    "for f in prediction_files:\n",
    "    data = pd.read_csv(f)\n",
    "    prediction_appended_data.append(data)\n",
    "\n",
    "df_prediction = pd.concat(prediction_appended_data)\n",
    "df_prediction.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Drop variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi2 = df_prediction.drop([\n",
    "    'userid', 'username','followed_by_viewer', 'igtvcount', 'blocked_by_viewer',\n",
    "    'follows_viewer', 'has_blocked_viewer', 'has_requested_viewer', 'external_url',\n",
    "    'is_verified', 'requested_by_viewer', 'profile_pic_url', 'similar_accounts', 'business_category_name', 'biography', 'full_name'\n",
    "],\n",
    "        axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = joblib.load('../../resources/preprocess.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online versus batch prediction\n",
    "\n",
    "The needs of your application dictate the type of prediction you should use.\n",
    "\n",
    "You should generally use online prediction (sometimes called HTTP prediction) when you are making requests in response to application input or in other situations where timely inference is needed.\n",
    "\n",
    "Batch prediction is ideal for processing accumulated data when you don't need immediate results. For example a periodic job that gets predictions for all data collected since the last job.\n",
    "\n",
    "In both cases, you pass input data to a cloud-hosted machine-learning model and get inferences for each data instance. The differences are shown as follows:\n",
    "\n",
    "#### Online prediction\t\n",
    "* Optimized to minimize the latency of serving predictions.\t\n",
    "* Can process one or more instances per request.\t\n",
    "* Predictions returned in the response message.\t\n",
    "* Input data passed directly as a JSON string.\t\n",
    "* Returns as soon as possible.\t\n",
    "\n",
    "\n",
    "#### Batch prediction\n",
    "\n",
    "* Optimized to handle a high volume of instances in a job and to run more complex models.\n",
    "* Can process one or more instances per request.\n",
    "* Predictions written to output files in a Cloud Storage location that you specify.\n",
    "* Input data passed indirectly as one or more URIs of files in Cloud Storage locations.\n",
    "* Asynchronous request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Online Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=[300, 1200, 500, 0, 0, 1]\n",
    "keys=['mediacount', 'followers', 'followees', 'is_private', 'is_business_account', 'has_public_story']\n",
    "dict6=dict(zip(keys, inputs))\n",
    "test=pd.DataFrame([dict6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test[['is_private', 'mediacount', 'followers', 'followees', 'is_business_account', 'has_public_story']]\n",
    "test_array = preprocess.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpickle the final model\n",
    "file = open('../../resources/final_model.pkl', 'rb')\n",
    "model=pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba(test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(test_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction for 1 profile to become a follower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Predictions\n",
    "\n",
    "Batch prediction latency\n",
    "If you use a simple model and a small set of input instances, you'll find that there is a considerable difference between how long it takes to finish identical prediction requests using online versus batch prediction. \n",
    "\n",
    "It might take a batch job several minutes to complete predictions that are returned almost instantly by an online request. This is a side-effect of the different infrastructure used by the two methods of prediction. \n",
    "\n",
    "Sagemaker allocates and initializes resources for a batch prediction job when you send the request. Online prediction is typically ready to process at the time of request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xi = preprocess.transform(dfi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions\n",
    "predictions_batch=model.predict(Xi)\n",
    "probabilities_batch = model.predict_proba(Xi)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probabilities_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of profiles to follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact = pd.DataFrame(model.predict(Xi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_profiles = pd.concat([df_prediction['username'], contact], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "follow_profiles = predicted_profiles[predicted_profiles[0] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "follow_profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for Model Retraining\n",
    "\n",
    "Model retraining should not result in a different model generating process. Rather retraining simply refers to re-running the process that generated the previously selected model on a new training set of data. The features, model algorithm, and hyperparameter search space should all remain the same. One way to think about this is that retraining doesn’t involve any code changes. It only involves changing the training data set.\n",
    "\n",
    "Rather than deploying a model once and moving on to another project, machine learning practitioners need to retrain their models if they find that the data distributions have deviated significantly from those of the original training set. \n",
    "\n",
    "While the frequency of retraining will vary from problem-to-problem, ML engineers can start with a simple strategy that retrains models on a periodic basis as new data arrives and evolve to more complex processes that quantify and react to model drift.\n",
    "\n",
    "This concept, known as model drift, can be mitigated but involves additional overhead in the forms of monitoring infrastructure, oversight, and process.\n",
    "\n",
    "Monitor model — Test your deployment to ensure that your model is still performing as expected on test data with respect to your evaluation metrics and things like inference speed.\n",
    "\n",
    "Evaluate new data — Using a model in production means you will frequently pass brand new data through the model that it has never been tested on. It’s important to perform evaluation and dig into specific samples to see how your model performs on any new data it encounters.\n",
    "\n",
    "Continue understanding model — Some errors and biases in your model can be deep-seated and take a long time to uncover. You need to continuously test and probe your model for various edge cases and trends that could cause problems if they were to be discovered by clients instead.\n",
    "\n",
    "Expand capabilities — Even if everything is working perfectly, it’s possible that the model isn’t increasing profits as much as you hoped. From adding new classes, developing new data streams, and making the model more efficient there are countless ways to expand the capabilities of your current model to make it even better. Any time you want to improve your system, you will need to restart the ML lifecycle to update your data, model, and evaluate it all to make sure your new features work as expected.\n",
    "\n",
    "[link](https://mlinproduction.com/model-retraining/#:~:text=Rather%20than%20deploying%20a%20model,of%20the%20original%20training%20set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_data = df_prediction[df_prediction['username'].isin(follow_profiles['username'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_time = datetime.now().strftime(\"%Y-%m-%d_%I-%M-%S_%p\")\n",
    "# file name\n",
    "ext =\".csv\"\n",
    "profile_name_path = str(filenames.profile_path) + file_time + ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_data.to_csv(profile_name_path, index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e88e18f642312201c56837a6b99762295fbd9da17f5aaf449f73ef004e8be5c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
