{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning life cycle is the cyclical process that data science projects follow. It defines each step that an organization should follow to take advantage of machine learning and artificial intelligence (AI) to derive practical business value.\n",
    "\n",
    "Why is the Machine Learning Life Cycle Important?\n",
    "\n",
    "The machine learning life cycle is important because it delineates the role of every person in a company in data science initiatives, ranging from business to engineering personnel. It takes each and every project from inception to completion and gives a high-level perspective of how an entire data science project should be structured in order to result in real, practical business value. Failing to accurately execute on any one of these steps will result in misleading insights or models with no practical value.\n",
    "\n",
    "\n",
    "There are five major steps in the machine learning life cycle, all of which have equal importance and go in a specific order.\n",
    "\n",
    "Break down how machine learning models are built into five steps: \n",
    "1. [Acquire and Explore Data](#Data)\n",
    "2. [Data preparation and exploration](#analysis)\n",
    "3. Model build and train \n",
    "4. Interpret and Communicate\n",
    "5. Implement, Document, and Maintain\n",
    "\n",
    "\n",
    "![Machine Learning Lifecycle](images\\ml-lifecycle.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import filenames\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_rows', 500, 'display.max_columns', 500,\n",
    "              'display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "1. Data Collection — Collect as much raw data as possible regardless of quality In the end, only a small subset of it will be annotated anyway which is where most of the cost comes from. It is useful to have a lot of data available to add as needed when problems arise with model performance.\n",
    "\n",
    "2. Define your annotation schema — This is one of the most important parts of the data phase of the lifecycle, and it often gets overlooked. A poorly constructed annotation schema will result in ambiguous classes and edge cases that make it much more difficult to train a model.\n",
    "\n",
    "3. Data Annotation—Annotation is a tedious process of performing the same task on and on for hours at a time, which is why annotation services are a booming business. The result is that annotators will likely make numerous mistakes. While most annotation firms guarantee a maximum error percentage (ex. 2% max error), a larger problem is a poorly defined annotation schema resulting in annotators deciding to label samples differently. This is harder to spot by the QA team of an annotation firm and is something that you need to check yourself.\n",
    "\n",
    "4. Improve dataset and annotations — You will likely spend the majority of your time here when trying to improve model performance. If your model is learning but not performing well, the culprit is almost always a training dataset containing biases and mistakes that are creating a performance ceiling for your model. Improving your model generally involves things like hard sample mining (adding new training data similar to other samples the model failed on), rebalancing your dataset based on biases your model has learned, and updating your annotations and schema to add new labels and refine existing ones.\n",
    "\n",
    "### You use glob to get all the csv files in the raw data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_files = filenames.profile_folder_path.glob(os.path.join(\"*.csv\"))\n",
    "\n",
    "profile_appended_data = []\n",
    "# loop over the list of csv files\n",
    "for f in profile_files:\n",
    "    data = pd.read_csv(f)\n",
    "    profile_appended_data.append(data)\n",
    "#profile_appended_data\n",
    "\n",
    "df = pd.concat(profile_appended_data)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop duplicate userid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['userid'], keep='last').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Label for Followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "fpath = filenames.followers_path\n",
    "follower = []\n",
    "with open(fpath, newline='') as f:\n",
    "    for i in csv.reader(f):\n",
    "        follower.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(follower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_follower'] = df['username'].isin(follower).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Username of extracted profiles to date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['username'].to_csv(filenames.todate_path, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Usernames of similar content for traveltrackie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stripping and splitting the list within a string without inverted commas\n",
    "# similar_accounts = (\n",
    "#     df.iloc[0]['similar_accounts']).strip(\"'['\").rstrip(\"']'\").split(\",\")\n",
    "\n",
    "# similar_accounts_list = []\n",
    "# for user in similar_accounts:\n",
    "#     similar_accounts_list.append(user.split(\" \")[2])\n",
    "\n",
    "# print(\"Storing similar accounts into file.\")\n",
    "# with open(similar_accounts_path, 'w') as f:\n",
    "#     for similar_account in similar_accounts_list:\n",
    "#         print(similar_account, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "After getting the data, data scientists have to prepare the raw data, perform data exploration, visualize data, transform data and possibly repeat the steps until it’s ready to use for modeling. Data preparation is cleansing and processing raw data before analysis. Before building any machine learning model, data scientists need to understand the available data. \n",
    "\n",
    "Raw data can be messy, duplicated or inaccurate. Data scientists explore the data available to them, then cleanse the data by identifying corrupt, inaccurate and incomplete data and replacing or deleting it.\n",
    "In addition, data scientists need to determine if the data has labels or not. For example, if you\n",
    "have a series of images and you want to develop a detection model to determine whether there is a car in the image, you need to have a set of images labeled whether there is a car in them and most likely need bounding boxes around the cars in the images. If the images lack labels, data scientists will have to label them. There are open source tools and commercial vendors that provide platforms for data labeling, as well as human labelers for hire. \n",
    "\n",
    "After data is cleansed, data scientists explore the features (or the variables) in their dataset, identify any relationship between the features transformations. There are various tools data\n",
    "scientists can use for exploratory data analysis in open source libraries and analytics/data science\n",
    "platforms. A tool that performs statistical analysis of the dataset and creates data visualizations to\n",
    "generate plots of the features is useful in this step.\n",
    "\n",
    "It is important to see what types of features are in the dataset. Features can be numerical, which\n",
    "can be a floating point or integer. Categorical features have a finite number of possible values,\n",
    "typically assigning data into groups. For example, if you have a dataset from a customer survey,\n",
    "the respondent’s gender (male or female) is a categorical feature. Ordinal features are a\n",
    "categorical feature with a set order or scale. \n",
    "\n",
    "For example, customer satisfaction response: very satisified, satisfied, indifferent, dissatisfied, and very dissatisfied has a set order to it. You can convert that ordering into an integer scale (1->5).\n",
    "After determining what kind of features there are, obtaining a distribution of values that each of the\n",
    "feature has and getting summary statistics of each feature would be next. Doing so would help answer\n",
    "the following questions about the dataset:\n",
    "\n",
    "* Is the dataset skewed towards a range of values or a subset of categories?\n",
    "* What are the minimum, maximum, mean, median and mode values of the feature?\n",
    "* Are there missing values or invalid values such as null? If so, how many are there?\n",
    "* Are there outliers in the dataset?\n",
    "\n",
    "During the data exploration step, it is helpful to plot the features and also plot the features\n",
    "against each other to identify patterns in the dataset. This helps to determine the need for data\n",
    "transformation. Some of the questions you need to answer are:\n",
    "\n",
    "* How do you handle missing values? Do you want to fill in the values and if so, what approach do you plan to take to fill in for the missing value? Some approaches include taking the mean value, the median, the mode, nearby entry’s value and average of nearby entries’ values.\n",
    "* How will you handle outliers?\n",
    "* Are some of your features correlated with each other?\n",
    "* Do you need to normalize the dataset or perform some other transformation to rescale the\n",
    "data (e.g. log transformation)?\n",
    "* What is your approach to a long tail of categorical values? Do you use them as-is, group them\n",
    "in some meaningful way or ignore a subset of them altogether? \n",
    "\n",
    "During the data exploration step, you can identify patterns in your dataset for ideas about how to \n",
    "develop new features that would better represent the dataset. This is known as feature engineering. \n",
    "For example, if you have a traffic dataset for the number of vehicles passing through a major \n",
    "intersection at every hour, you might want to create a new feature categorizing the hour into \n",
    "different parts of the day, such as early morning, mid-morning, early afternoon, late afternoon, \n",
    "and nighttime.\n",
    "\n",
    "For categorical features, often it is necessary to one hot encode the feature. One hot encoding \n",
    "means turning a categorical feature into binary features, one for each of the categories. \n",
    "For example, suppose you have a dataset of customers, and we have a feature on which states \n",
    "the customer comes from: Washington, Oregon, \n",
    "— 0.30 \n",
    "— 0.25\n",
    "— 0.20 \n",
    "— 0.15\n",
    "— 0.10\n",
    "— 0.05\n",
    "— 0.00 \n",
    "Heatmap of how correlated the features are to each other, from a dataset with three types of wine and features of each wine. and California. One hot encoding would produce two binary features where one feature is whether a customer is from Washington state or not, and the second feature is whether a customer is from Oregon or not. It is assumed that if the customer is not from Washington or Oregon, he / she would be from California, so there is no need for a third feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def describe_dataframe(df=pd.DataFrame()):\n",
    "    \"\"\"This function generates descriptive stats of a dataframe\n",
    "    Args:\n",
    "        df (dataframe): the dataframe to be analyzed\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"*\" * 30)\n",
    "    print(\"About the Data\")\n",
    "    print(\"*\" * 30)\n",
    "\n",
    "    print(\"Number of rows::\", df.shape[0])\n",
    "    print(\"Number of columns::\", df.shape[1])\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Column Names::\", df.columns.values.tolist())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Column Data Types::\\n\", df.dtypes)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Columns with Missing Values::\",\n",
    "          df.columns[df.isnull().any()].tolist())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"General Stats::\")\n",
    "    print(df.info())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Summary Stats::\")\n",
    "    print(df.describe())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Dataframe Sample Rows::\")\n",
    "    display(df.head(3))\n",
    "    \n",
    "describe_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have external_url and business_category_name with the most values missing. biography and full_name are the next two columns with the most values missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The heatmap shows a positive correlation with blue. The darker the shade of blue, the more the correlation.\n",
    "msno.heatmap(df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These bars show the values that are proportional to the non-missing data in the dataset. Along with that, the number of values missing is also shown.\n",
    "msno.bar(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUALITATIVE VARIABLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Counts for multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\n",
    "    'is_private', 'followed_by_viewer', 'is_business_account',\n",
    "    'blocked_by_viewer', 'follows_viewer', 'has_blocked_viewer',\n",
    "    'has_public_story', 'has_requested_viewer', 'is_verified',\n",
    "    'requested_by_viewer', 'is_follower'\n",
    "]].apply(pd.Series.value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Count for normalized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart\n",
    "df['business_category_name'].value_counts(normalize=True)#.plot(kind='pie')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['business_category_name'].value_counts(normalize=True).plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUANTITATIVE VARIABLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimum Number of Data Bins\n",
    "With histograms, there are rules for determining the optimum number of bins (classes or intervals) into which a distribution of observations should be grouped. For example, the Sturges rule (1926) considers the optimum number of bins to be:\n",
    "\n",
    " k=[1+log2(n)] \n",
    "\n",
    "where n is the sample size.\n",
    "\n",
    "* https://stackoverflow.com/questions/3719631/log-to-the-base-2-in-python/28033134"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def optimum_bins(df, col):\n",
    "    return math.frexp(df[col].count())[1]\n",
    "\n",
    "\n",
    "def optimum_bins_2(df, col, num):\n",
    "    return math.frexp(df[df[col] <= num][col].count())[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Histogram of Media Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mediacount'].hist(bins=optimum_bins(df, 'mediacount'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prettier Histogram\n",
    "df[df.mediacount <= 1000][\"mediacount\"].hist(density=True,\n",
    "                                             bins=optimum_bins_2(\n",
    "                                                 df, 'mediacount', 1000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Histogram of  IGtv Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "df['igtvcount'].hist(bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Histogram of  Followees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Histogram\n",
    "df['followees'].hist(bins=optimum_bins(df, 'followees'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prettier Histogram\n",
    "df[df.followees <= 4000][\"followees\"].hist(density=True,\n",
    "                                           bins=optimum_bins_2(\n",
    "                                               df, 'followees', 4000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Histogram of  Followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Histogram\n",
    "df['followers'].hist(bins=optimum_bins(df, 'followers'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prettier Histogram\n",
    "df[df.followers <= 6000][\"followers\"].hist(density=True,\n",
    "                                           bins=optimum_bins_2(\n",
    "                                               df, 'followers', 6000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detection of Outliers\n",
    "The outliers in the dataset can be detected by the below methods:\n",
    "\n",
    "* Z-score\n",
    "* Scatter Plots\n",
    "* Interquartile range(IQR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[['mediacount']].boxplot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['mediacount', 'is_follower']].boxplot(by='is_follower');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[['followers']].boxplot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2021/05/feature-engineering-how-to-detect-and-remove-outliers-with-python-code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.subplot(1,2,1)\n",
    "sns.distplot(df['followers'])\n",
    "plt.subplot(1,2,2)\n",
    "sns.distplot(df['mediacount'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['followers', 'is_follower']].boxplot(by='is_follower');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[['followees']].boxplot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['followees'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['followees', 'is_follower']].boxplot(by='is_follower');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mediacount, Followers and Followees all are right skewed. WE are going to use IQR method to remove the outliers in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(df['profile_pic_url'][0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['external_url'].dropna()[0:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not need 'followed_by_viewer',  'blocked_by_viewer', 'follows_viewer', 'has_blocked_viewer', 'has_requested_viewer', 'is_verified', 'requested_by_viewer' as these have no variability. We also do not need userid, igtvcount, and  similar accounts. There is no information from profile_pic_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\n",
    "    'userid', 'followed_by_viewer', 'igtvcount', 'blocked_by_viewer',\n",
    "    'follows_viewer', 'has_blocked_viewer', 'has_requested_viewer',\n",
    "    'is_verified', 'requested_by_viewer', 'profile_pic_url', 'similar_accounts'\n",
    "],\n",
    "        axis=1,\n",
    "        inplace=True)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e88e18f642312201c56837a6b99762295fbd9da17f5aaf449f73ef004e8be5c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
